# SENTIMENT ANALYSIS PROJECT – U.S. SENATORS’ TWEETS (May–Oct 2020)  
Hello Professor,

## 1) Project topic and why it matters
This project studies sentiment in tweets posted by U.S. senators. Senators use Twitter/X to communicate policies, react to events, and mobilize supporters. From a scientific/NLP view, tweets are short and noisy (hashtags, mentions, slang), so they are a good real-world test for sentiment analysis. From a “business” view, the same type of analysis is useful for media monitoring, reputation tracking, and understanding which communication styles get more engagement.

## 2) Objective 
Using senators’ tweets from **May 1 to Oct 31, 2020**, my goal is to:

- Label each tweet as **Negative / Neutral / Positive** using:
  - **(A) Dictionary / corpus-based methods:**  
    - **VADER** (lexicon + rules baseline)  
    - **Custom corpus lexicon (Seed + PMI)** (lexicon expanded from this dataset)  
  - **(B) Supervised classical ML:** TF-IDF + Logistic Regression + GridSearchCV  
  - **(C) Transformer model:** `cardiffnlp/twitter-roberta-base-sentiment-latest`
- Compare methods and explain differences
- Analyze sentiment patterns by time (monthly), party, and engagement (likes/retweets/replies)
- Present concise conclusions and limitations

## 3) Data
**Main dataset**
- `senator_twitter_May-Oct.csv`  
- Source: Harvard Dataverse – [Dataset link](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/K4XSYC)

Key fields include tweet text, date/time, engagement (likes/retweets/replies), and metadata (senator name, state, party, etc.).  
**Time range:** May–Oct 2020.

**Output files created during the project (for convenience)**
- `senator_tweets_with_sentiment_plus_vader.csv` *(dictionary method results)*
- `senator_tweets_with_sentiment.csv` *(transformer results)*
- `senator_tweets_with_lr_tfidf.csv` *(TF-IDF + LR supervised results)*

## 4) Notebooks (report structure)
I organized the report as **three notebooks**, one per method family, because each method has different preprocessing and outputs.

### A) `senator_sentment_dictionary.ipynb`
**Dictionary / corpus-based methods:** VADER + Custom corpus lexicon (Seed + PMI)  
- **Purpose:** fast and interpretable baselines for social-media text  
- **Outputs:**  
  - VADER: `neg/neu/pos/compound` + 3-class labels  
  - Custom lexicon: PMI-expanded lexicon scores (raw + normalized) + 3-class labels

### B) `senator_sentiment_transformers.ipynb`
**Transformer model:** `cardiffnlp/twitter-roberta-base-sentiment-latest`  
- **Purpose:** strong modern model trained for Twitter-style sentiment  
- **Output:** label probabilities, confidence, final 3-class label, sentiment score

### C) `senator_sentiment_TF-IDF+LR+CV.ipynb`
**Supervised classical ML:** TF-IDF + Logistic Regression + GridSearchCV  
- **Purpose:** train a supervised classifier and optimize hyperparameters  
- **IMPORTANT labeling note (silver labels):**  
  The dataset does not include human sentiment labels, so for supervised learning I used **“silver labels”** generated by the Transformer model. That means the supervised model is trained to approximate the Transformer’s sentiment labels, not human ground truth. I mention this clearly because it affects how to interpret evaluation metrics.

## 5) Methods (what I did)

### 5.1 Dictionary / corpus-based methods
#### 5.1.1 Custom corpus lexicon (Seed + PMI expansion)
- Start from a small seed dictionary of positive/negative words
- Expand the lexicon using corpus co-occurrence statistics (PMI) on the senators’ tweets
- Score each tweet by summing lexicon weights; also compute a **normalized score** to reduce tweet-length effects
- Map scores to Negative/Neutral/Positive using thresholds  
- **Note:** this method can be sensitive to hashtags and topic-related words, so I interpret it as an additional baseline (not ground truth)
#### 5.1.2 VADER (lexicon + rules baseline)
- Light preprocessing (keep punctuation/emojis where possible) because VADER uses rules based on them  
- Generate compound score and map to Negative/Neutral/Positive using standard thresholds
### 5.2 Transformer model
- Minimal cleaning suitable for transformer tokenization (remove noise but keep meaningful text)  
- Run inference in batches; store class probabilities and confidence  
- Compute summary statistics by party/month and compare with engagement

### 5.3 Supervised ML (TF-IDF + Logistic Regression)
- Preprocessing: stronger normalization (e.g., remove URLs/mentions as features, lowercasing, etc.)  
- Use a pipeline: **TF-IDF vectorizer → LogisticRegression**  
- Hyperparameter optimization: GridSearchCV over settings such as n-gram range, min_df/max_df, sublinear_tf, C, and class_weight  
- Evaluate on a held-out test split *(again: this is agreement with silver labels, not true sentiment accuracy)*

## 6) Results summary (main comparison)
Main label distributions observed in this project (approximate):

- **VADER:**        ~15.7% Negative / 20.0% Neutral / 64.7% Positive  
- **Transformers:** ~10.2% Negative / 49.6% Neutral / 40.2% Positive  
- **TF-IDF + LR:**  ~10.8% Negative / 49.2% Neutral / 40.5% Positive  
- **Custom lexicon (Seed + PMI):** ~3.2% Negative / 12.3% Neutral / 84.5% Positive  

**Agreement**
- Transformers vs TF-IDF + LR: ~98.7% agreement *(expected because LR learned from Transformer silver labels)*  
- VADER vs Transformers:       ~58.5% agreement *(VADER behaves differently; more “positive-heavy”)*  
- **Custom lexicon vs others:** lower agreement is expected because the method is corpus-driven and produced a strongly Positive-skewed distribution.

**Supervised evaluation (on the silver-labeled split)**
- TF-IDF + LR best model test accuracy ≈ 0.819  
- Macro F1 ≈ 0.782  
**Interpretation:** this reflects how well LR reproduces transformer labels, not real-world sentiment correctness.

## 7) Project Summary & Conclusions (final report-style section)

### 7.1 What I learned from comparing methods
- VADER is quick and interpretable, but it labels many tweets as Positive. In political tweets, “positive” words can appear in strategic messaging, slogans, or sarcasm, so a lexicon can over-estimate positivity.  
- The custom corpus lexicon adapts to the dataset vocabulary, but it can **over-score hashtags and recurring political phrases**, leading to a very Positive-skewed distribution. This is why I treat it as an extra baseline and interpret its results cautiously.  
- The Transformer model produces many more Neutral predictions. That makes sense because many political tweets are informational (announcements, updates, links) rather than strongly emotional.  
- TF-IDF + LR matches the Transformer distribution closely. This is expected because the LR model was trained on Transformer-generated labels. The benefit is speed: once trained, LR is much cheaper/faster to run.

### 7.2 Sentiment patterns 
- **Party differences:** sentiment distributions differ by party, but the direction and size depend on the method. This is exactly why comparing methods matters.  
- **Time trends:** monthly sentiment varies, which may reflect news cycles/events. (In a future improvement, I would connect large sentiment shifts to specific events more directly.)  
- **Engagement:** sentiment alone shows weak correlations with engagement. Engagement is influenced by many other factors (account size, topics, virality, media coverage), so sentiment alone does not “cause” engagement.

### 7.3 Business/scientific value of the results
- **Business-style use case:** a communications team could monitor sentiment shifts in official messaging and see whether a more neutral vs emotional tone is associated with higher engagement.  
- **Scientific/NLP use case:** the project demonstrates how different sentiment methods behave on short political texts and why evaluation strategy matters.

### 7.4 Limitations 
- **No human ground-truth labels:** the largest limitation is evaluation validity. With silver labels, the supervised model is evaluated mainly on agreement with the Transformer model.  
- **Sarcasm and context:** political tweets can be sarcastic or rely on external context (links, quoted text).  
- **Topic confounding:** sentiment can correlate with topic (e.g., crisis-related tweets), which can also affect engagement.  
- **Lexicon bias:** dictionary/corpus methods may overreact to hashtags or common political phrases and may miss context-dependent sentiment.

## 8) How to run (quick guide)
1. Open the notebooks in Jupyter:
   - `senator_sentment_dictionary.ipynb`
   - `senator_sentiment_transformers.ipynb`
   - `senator_sentiment_TF-IDF+LR+CV.ipynb`
2. Make sure `senator_twitter_May-Oct.csv` is in the same folder (or adjust the path)
3. Run cells from top to bottom. Each notebook saves/uses intermediate CSV outputs.

That’s it — this README explains the structure, methods, and final conclusions in a way that matches the project requirements.
