# SENTIMENT ANALYSIS PROJECT – U.S. SENATORS’ TWEETS (May–Oct 2020)  
Hello proffesor,

## 1) Project topic and why it matters
This project studies sentiment in tweets posted by U.S. senators. Senators use Twitter/X to communicate policies, react to events, and mobilize supporters. From a scientific/NLP view, tweets are short and noisy (hashtags, mentions, slang), so they are a good real-world test for sentiment analysis. From a “business” view, the same type of analysis is useful for media monitoring, reputation tracking, and understanding which communication styles get more engagement.

## 2) Objective 
Using senators’ tweets from **May 1 to Oct 31, 2020**, my goal is to:

- Label each tweet as **Negative / Neutral / Positive** using:  
  - **(A) Dictionary/corpus-based method:** VADER  
  - **(B) Supervised classical ML:** TF-IDF + Logistic Regression + GridSearchCV  
  - **(C) Transformer model:** `cardiffnlp/twitter-roberta-base-sentiment-latest`
- Compare methods and explain differences
- Analyze sentiment patterns by time (monthly), party, and engagement (likes/retweets/replies)
- Present concise conclusions and limitations

## 3) Data
**Main dataset**
- `senator_twitter_May-Oct.csv`  
- Source: Harvard Dataverse – [Dataset link](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/K4XSYC)

Key fields include tweet text, date/time, engagement (likes/retweets/replies), and metadata (senator name, state, party, etc.).  
**Time range:** May–Oct 2020.

**Output files created during the project (for convenience)**
- `senator_tweets_with_sentiment_plus_vader.csv` *(dictionary method results)*
- `senator_tweets_with_sentiment.csv` *(transformer results)*
- `senator_tweets_with_lr_tfidf.csv` *(TF-IDF + LR supervised results)*

## 4) Notebooks (report structure)
I organized the report as **three notebooks**, one per method, because each method has different preprocessing and outputs.

### A) `senator_sentment_dictionary.ipynb`
**Dictionary/corpus-based method:** VADER  
- **Purpose:** fast and interpretable baseline for social-media text  
- **Output:** VADER `neg/neu/pos/compound` and 3-class labels

### B) `senator_sentiment_transformers.ipynb`
**Transformer model:** `cardiffnlp/twitter-roberta-base-sentiment-latest`  
- **Purpose:** strong modern model trained for Twitter-style sentiment  
- **Output:** label probabilities, confidence, final 3-class label, sentiment score

### C) `senator_sentiment_TF-IDF+LR+CV.ipynb`
**Supervised classical ML:** TF-IDF + Logistic Regression + GridSearchCV  
- **Purpose:** train a supervised classifier and optimise hyperparameters  
- **IMPORTANT labeling note (silver labels):**  
  The dataset does not include human sentiment labels, so for supervised learning I used **“silver labels”** generated by the Transformer model. That means the supervised model is trained to approximate the Transformer’s sentiment labels, not human ground truth. I mention this clearly because it affects how to interpret evaluation metrics.

## 5) Methods (what I did)

### 5.1 Dictionary/corpus-based: VADER
- Light preprocessing (keep punctuation/emojis where possible) because VADER uses rules based on them  
- Generate compound score and map to Negative/Neutral/Positive using standard thresholds

### 5.2 Transformer model
- Minimal cleaning suitable for transformer tokenization (remove noise but keep meaningful text)  
- Run inference in batches; store class probabilities and confidence  
- Compute summary statistics by party/month and compare with engagement

### 5.3 Supervised ML (TF-IDF + Logistic Regression)
- Preprocessing: stronger normalization (e.g., remove URLs/mentions as features, lowercasing, etc.)  
- Use a pipeline: **TF-IDF vectorizer → LogisticRegression**  
- Hyperparameter optimisation: GridSearchCV over settings such as n-gram range, min_df/max_df, sublinear_tf, C, and class_weight  
- Evaluate on a held-out test split *(again: this is agreement with silver labels, not true sentiment accuracy)*

## 6) Results summary (main comparison)
Main label distributions observed in this project (approximate):

- **VADER:**        ~15.7% Negative / 20.0% Neutral / 64.7% Positive  
- **Transformers:** ~10.2% Negative / 49.6% Neutral / 40.2% Positive  
- **TF-IDF + LR:**  ~10.8% Negative / 49.2% Neutral / 40.5% Positive  

**Agreement**
- Transformers vs TF-IDF + LR: ~98.7% agreement *(expected because LR learned from Transformer silver labels)*  
- VADER vs Transformers:       ~58.5% agreement *(VADER behaves differently; more “positive-heavy”)*  

**Supervised evaluation (on the silver-labeled split)**
- TF-IDF + LR best model test accuracy ≈ 0.819  
- Macro F1 ≈ 0.782  
**Interpretation:** this reflects how well LR reproduces transformer labels, not real-world sentiment correctness.

## 7) Project Summary & Conclusions (final report-style section)

### 7.1 What I learned from comparing methods
- VADER is quick and interpretable, but it labels many tweets as Positive. In political tweets, “positive” words can appear in strategic messaging, slogans, or sarcasm, so a lexicon can over-estimate positivity.  
- The Transformer model produces many more Neutral predictions. That makes sense because many political tweets are informational (announcements, updates, links) rather than strongly emotional.  
- TF-IDF + LR matches the Transformer distribution closely. This is expected because the LR model was trained on Transformer-generated labels. The benefit is speed: once trained, LR is much cheaper/faster to run.

### 7.2 Sentiment patterns 
- **Party differences:** sentiment distributions differ by party, but the direction and size depend on the method. This is exactly why comparing methods matters.  
- **Time trends:** monthly sentiment varies, which may reflect news cycles/events. (In a future improvement, I would connect large sentiment shifts to specific events more directly.)  
- **Engagement:** tweets with stronger sentiment (more positive or more negative) often show different engagement patterns compared to neutral tweets. However, engagement is influenced by many other factors (account size, topics, virality, media coverage), so sentiment alone does not “cause” engagement.

### 7.3 Business/scientific value of the results
- **Business-style use case:** a communications team could monitor sentiment shifts in official messaging and see whether a more neutral vs emotional tone is associated with higher engagement.  
- **Scientific/NLP use case:** the project demonstrates how different sentiment methods behave on short political texts and why evaluation strategy matters.

### 7.4 Limitations 
- **No human ground-truth labels:** the largest limitation is evaluation validity. With silver labels, the supervised model is evaluated mainly on agreement with Transformer model .  
- **Sarcasm and context:** political tweets can be sarcastic or rely on external context (links, quoted text).  
- **Topic confounding:** sentiment can correlate with topic (e.g., crisis-related tweets), which can also affect engagement.

## 8) How to run (quick guide)
1. Open the three notebooks in Jupyter:
   - `senator_sentment_dictionary.ipynb`
   - `senator_sentiment_transformers.ipynb`
   - `senator_sentiment_TF-IDF+LR+CV.ipynb`
2. Make sure `senator_twitter_May-Oct.csv` is in the same folder (or adjust the path)
3. Run cells from top to bottom. Each notebook saves/uses intermediate CSV outputs.

That’s it — this README explains the structure, methods, and final conclusions in a way that matches the project requirements.
